{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# 首先加载必用的库\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import gensim  用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0711 09:22:06.416603  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:11.286582  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:22.673138  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:30.453337  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:33.003519  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:33.727582  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:34.623189  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:34.668067  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:37.058677  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:37.284072  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:38.797028  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:39.124153  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:39.146094  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:40.342895  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.098874  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.115828  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.223540  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.299337  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.372143  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.549669  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.578593  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.584575  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.616490  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.683310  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.706251  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.792020  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.864825  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.875796  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.925662  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:41.943614  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.007445  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.039359  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.048335  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.052326  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.063296  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.068282  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.071274  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0711 09:22:42.076260  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.093215  6504 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.098202  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.112165  6504 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.113162  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.133109  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.135104  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.145076  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.147071  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.158042  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.170009  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.173999  6504 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.178986  6504 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.186966  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.188959  6504 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0711 09:22:42.192949  6504 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n"
     ]
    }
   ],
   "source": [
    "# 使用gensim加载预训练中文分词embedding, 有可能需要等待1-2分钟\n",
    "cn_model = KeyedVectors.load_word2vec_format('../embeddings/sgns.zhihu.bigram',binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由此可见每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['山东大学'].shape[0]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数：10526\n"
     ]
    }
   ],
   "source": [
    "# 获得样本的索引\n",
    "import pandas as pd\n",
    "\n",
    "data_neg = pd.read_excel('../data/neg9.xlsx')\n",
    "# data_pos = pd.read_excel('../data/pos.xls')\n",
    "\n",
    "print('样本总数：'+str(len(data_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>做为一本声名在外的流行书，说的还是广州的外企，按道理应该和我的生存环境差不多啊。但是一看之下...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lable                                               data\n",
       "0      1  做为一本声名在外的流行书，说的还是广州的外企，按道理应该和我的生存环境差不多啊。但是一看之下..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_neg.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有的评价内容放置到一个list里\n",
    "train_texts_orig = []\n",
    "# 文本所对应的labels，也就是标记\n",
    "train_target = []\n",
    "\n",
    "for indexs in data_neg.index:\n",
    "    train_texts_orig.append(data_neg.loc[indexs].values[1])\n",
    "    train_target.append(data_neg.loc[indexs].values[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'明明在携程上用信用卡担保定了房，入住时居然前台说没有此信息，而且房间已经住满了，打电话到携程投诉，说是酒店预订部与前台没沟通好，折腾了半天，非常气人!'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_texts_orig))\n",
    "# print(train_target)\n",
    "train_texts_orig[3824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_texts_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for indexs in data_pos.index:\n",
    "#     train_texts_orig.append(data_pos.loc[indexs].values[0])\n",
    "#     train_target.append(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_texts_orig))\n",
    "# # print(train_target)\n",
    "# train_texts_orig[3856]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有7909个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = str(text)\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（） ]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens: [76 34 33 ...  4  4  3]\n",
      "10526\n"
     ]
    }
   ],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print(\"num_tokens:\",num_tokens)\n",
    "# print(train_tokens)\n",
    "len(train_tokens)\n",
    "print(len(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.46817404522135\n"
     ]
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "print(np.mean(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085\n"
     ]
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "print(np.max(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins = 100)\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9595287858635759"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为223时，大约96%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'再'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.index2word[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59,\n",
       " 69,\n",
       " 0,\n",
       " 154664,\n",
       " 1,\n",
       " 2010,\n",
       " 473,\n",
       " 16,\n",
       " 1,\n",
       " 57,\n",
       " 2537,\n",
       " 1,\n",
       " 11119,\n",
       " 649,\n",
       " 1305,\n",
       " 144,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 19945,\n",
       " 869,\n",
       " 90,\n",
       " 86,\n",
       " 197235,\n",
       " 126,\n",
       " 155,\n",
       " 39283,\n",
       " 22,\n",
       " 18,\n",
       " 711,\n",
       " 3,\n",
       " 40,\n",
       " 155,\n",
       " 574,\n",
       " 1,\n",
       " 87,\n",
       " 1946,\n",
       " 8,\n",
       " 34,\n",
       " 655,\n",
       " 1,\n",
       " 644,\n",
       " 4253,\n",
       " 18,\n",
       " 38,\n",
       " 16,\n",
       " 143,\n",
       " 13176,\n",
       " 78,\n",
       " 49,\n",
       " 0,\n",
       " 51,\n",
       " 27,\n",
       " 135,\n",
       " 29865,\n",
       " 391,\n",
       " 18,\n",
       " 711,\n",
       " 3,\n",
       " 91,\n",
       " 51,\n",
       " 202,\n",
       " 39,\n",
       " 11119,\n",
       " 4862,\n",
       " 122,\n",
       " 62,\n",
       " 441,\n",
       " 1,\n",
       " 188009,\n",
       " 691,\n",
       " 613,\n",
       " 4,\n",
       " 374,\n",
       " 7294,\n",
       " 79]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'做为 声名在外的流行书说的还是广州的外企按道理应该和我的生存环境差不多啊但是一看之下才发现相去甚远这也就算了还发现其中的很多规则有很强的企业个性也就说只是个例而不是 给我们这些老油条看看也就算了如果给那些对外企向往或者想了解的freshman来看实在是容易误导他们'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse = reverse_tokens(train_tokens[0])\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只使用前50000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.017840e-01, -1.653400e-01,  3.050800e-02, ...,  1.065250e-01,\n",
       "         5.534360e-01,  4.366500e-01],\n",
       "       [-6.517470e-01,  5.359700e-01,  3.402710e-01, ...,  8.053990e-01,\n",
       "         1.045930e-01,  1.936940e-01],\n",
       "       [-4.123210e-01,  2.282610e-01,  2.071140e-01, ...,  8.087770e-01,\n",
       "         5.675100e-02,  4.523740e-01],\n",
       "       ...,\n",
       "       [ 5.849840e-01,  1.121180e-01, -6.938330e-01, ..., -3.760570e-01,\n",
       "         1.203500e-01, -1.059511e+00],\n",
       "       [ 1.511710e-01, -3.200000e-04, -3.885760e-01, ..., -5.988550e-01,\n",
       "         4.273530e-01, -3.922630e-01],\n",
       "       [-4.536090e-01, -1.813600e-02, -1.306600e-01, ..., -6.608000e-02,\n",
       "         3.566680e-01,  3.898050e-01]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum(cn_model[cn_model.index2word[30]] == embedding_matrix[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix.shape (50000, 300)\n"
     ]
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "print(\"embedding_matrix.shape\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0, 18229,     1,   832,    18,\n",
       "         447,  1357,   421,  1148,  9827,     1,  8108,     1,    34,\n",
       "        8621,   203,    10,   779,     0,  1087,    86,  1357,    57,\n",
       "        4593,    86,  1704,    24,    96,  7737,     1, 13729,   112,\n",
       "       57734,    34,   369,   989])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens, padding='pre', truncating='pre')\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[train_pad >= num_words] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pad[33] [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0 18229     1   832    18   447  1357   421\n",
      "  1148  9827     1  8108     1    34  8621   203    10   779     0  1087\n",
      "    86  1357    57  4593    86  1704    24    96  7737     1 13729   112\n",
      "     0    34   369   989]\n"
     ]
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "print(\"train_pad[33]\", train_pad[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target向量\n",
    "train_target = np.array(train_target)\n",
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\"\"\" one-hot处理标签 \"\"\"\n",
    "train_target = to_categorical(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10526, 9)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, train_target, test_size=0.15, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize_sequences(squences, dimension=10):\n",
    "#     \"\"\"\n",
    "#     @函数功能:将序列向量化，初始化全0的序列，在单词索引对应的位置上置1\n",
    "#     \"\"\"\n",
    "#     resluts = np.zeros((len(squences), dimension))\n",
    "#     for i, sequence in enumerate(squences):\n",
    "#         resluts[i, sequence] = 1\n",
    "#     return resluts\n",
    "\n",
    "# X_train = vectorize_sequences(X_train)\n",
    "# X_test = vectorize_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              帮朋友买的他家小朋友应该会喜欢吧纸张的质量相比在书城里看的要差点不过价格要便宜些嘛还要提出来的就是等了好久快递的速度真的不咋样啊\n",
      "class:  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[30]))\n",
    "print('class: ', y_train[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 模型第一层为embedding\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_tokens, trainable=False))\n",
    "\n",
    "# model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "# model.add(LSTM(units=16, return_sequences=False))\n",
    "model.add(LSTM(units=32, return_sequences=False))\n",
    "\n",
    "model.add(Dense(9, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 184, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 15,042,921\n",
      "Trainable params: 42,921\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 我们来看一下模型的结构，一共90k左右可训练的变量\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint_Class9_V1.0.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "\n",
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-8, patience=0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping,\n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7157 samples, validate on 1790 samples\n",
      "Epoch 1/20\n",
      "7040/7157 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9682\n",
      "Epoch 00001: val_loss improved from 0.27072 to 0.19325, saving model to sentiment_checkpoint_Class9_V1.0.keras\n",
      "7157/7157 [==============================] - 33s 5ms/sample - loss: 0.0914 - acc: 0.9683 - val_loss: 0.1933 - val_acc: 0.9291\n",
      "Epoch 2/20\n",
      "7040/7157 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9686\n",
      "Epoch 00002: val_loss did not improve from 0.19325\n",
      "7157/7157 [==============================] - 39s 5ms/sample - loss: 0.0914 - acc: 0.9683 - val_loss: 0.1933 - val_acc: 0.9291\n",
      "Epoch 3/20\n",
      "7040/7157 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9680\n",
      "Epoch 00003: val_loss did not improve from 0.19325\n",
      "7157/7157 [==============================] - 33s 5ms/sample - loss: 0.0914 - acc: 0.9683 - val_loss: 0.1933 - val_acc: 0.9291\n",
      "Epoch 4/20\n",
      "7040/7157 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9680\n",
      "Epoch 00004: val_loss did not improve from 0.19325\n",
      "7157/7157 [==============================] - 31s 4ms/sample - loss: 0.0914 - acc: 0.9683 - val_loss: 0.1933 - val_acc: 0.9291\n",
      "Epoch 5/20\n",
      "7040/7157 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9679\n",
      "Epoch 00005: val_loss improved from 0.19325 to 0.19325, saving model to sentiment_checkpoint_Class9_V1.0.keras\n",
      "7157/7157 [==============================] - 32s 5ms/sample - loss: 0.0914 - acc: 0.9683 - val_loss: 0.1933 - val_acc: 0.9291\n",
      "Epoch 6/20\n",
      "7040/7157 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9682\n",
      "Epoch 00006: val_loss did not improve from 0.19325\n",
      "7157/7157 [==============================] - 36s 5ms/sample - loss: 0.0914 - acc: 0.9683 - val_loss: 0.1933 - val_acc: 0.9291\n",
      "Epoch 7/20\n",
      "7040/7157 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9679\n",
      "Epoch 00009: val_loss did not improve from 0.19325\n",
      "7157/7157 [==============================] - 36s 5ms/sample - loss: 0.0914 - acc: 0.9683 - val_loss: 0.1933 - val_acc: 0.9291\n",
      "Epoch 10/20\n",
      "7040/7157 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9683\n",
      "Epoch 00010: val_loss did not improve from 0.19325\n",
      "7157/7157 [==============================] - 34s 5ms/sample - loss: 0.0914 - acc: 0.9683 - val_loss: 0.1933 - val_acc: 0.9291\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "history = model.fit(X_train, y_train,\n",
    "          validation_split=0.2,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1579/1579 [==============================] - 4s 2ms/sample - loss: 0.2446 - acc: 0.9120\n",
      "Accuracy:91.20%\n"
     ]
    }
   ],
   "source": [
    "# 开始测试\n",
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.200107738538055,\n",
       "  0.19969674650808275,\n",
       "  0.19925683958147464,\n",
       "  0.19865159010808067,\n",
       "  0.19859221115182346,\n",
       "  0.1985863287339869,\n",
       "  0.1985860960273431],\n",
       " 'acc': [0.93117744,\n",
       "  0.9315328,\n",
       "  0.93117744,\n",
       "  0.93200666,\n",
       "  0.93200666,\n",
       "  0.93200666,\n",
       "  0.93200666],\n",
       " 'val_loss': [0.32087333194110085,\n",
       "  0.32065054122061015,\n",
       "  0.32096215781058496,\n",
       "  0.32093525992922634,\n",
       "  0.3209306509794392,\n",
       "  0.3209310137171842,\n",
       "  0.32093077793288916],\n",
       " 'val_acc': [0.88178915,\n",
       "  0.8828541,\n",
       "  0.88178915,\n",
       "  0.88178915,\n",
       "  0.88178915,\n",
       "  0.88178915,\n",
       "  0.88178915],\n",
       " 'lr': [1.0000001e-05,\n",
       "  1.0000001e-05,\n",
       "  1.0000001e-05,\n",
       "  1.0000001e-06,\n",
       "  1.0000001e-07,\n",
       "  1.0000001e-08,\n",
       "  1e-08]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_acc(history):\n",
    "    \"\"\" 绘制精度曲线 \"\"\"\n",
    "    plt.clf()\n",
    "    history_dict = history.history\n",
    "    acc = history_dict['acc']\n",
    "    val_acc = history_dict['val_acc']\n",
    "\n",
    "    epochs = range(1, len(val_acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" show result \"\"\"\n",
    "show_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens, padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    num = np.argmax(result)\n",
    "    num += 1\n",
    "    if num >= 0 and num < 9:\n",
    "        print('负面评价  %d' % num, 'output=%.2f' % result[0][num])\n",
    "    else:\n",
    "        print('error!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度很不好\n",
      "负面评价  3 output=0.01\n",
      "酒店卫生条件非常不好\n",
      "负面评价  7 output=0.05\n",
      "床铺非常舒适\n",
      "负面评价  7 output=0.05\n",
      "房间很凉，不给开暖气\n",
      "负面评价  7 output=0.07\n",
      "房间很凉爽，空调冷气很足\n",
      "负面评价  7 output=0.04\n",
      "酒店环境不好，住宿体验很不好\n",
      "负面评价  6 output=0.23\n",
      "房间隔音不到位\n",
      "负面评价  7 output=0.04\n",
      "晚上回来发现没有打扫卫生\n",
      "负面评价  6 output=0.14\n",
      "因为过节所以要我临时加钱，比团购的价格贵\n",
      "负面评价  6 output=0.14\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度很不好',\n",
    "    '酒店卫生条件非常不好',\n",
    "    '床铺非常舒适',\n",
    "    '房间很凉，不给开暖气',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生',\n",
    "    '因为过节所以要我临时加钱，比团购的价格贵'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-7b564eda5e81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"明明在携程上用信用卡担保定了房，入住时居然前台说没有此信息，而且房间已经住满了，打电话到携程投诉，说是酒店预订部与前台没沟通好，折腾了半天，非常气人!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[1;31m# generate symbolic tensors).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1060\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    332\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     data = [\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m     ]\n\u001b[0;32m    336\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    332\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     data = [\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m     ]\n\u001b[0;32m    336\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[1;34m(x, expected_shape)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m   if (x.shape is not None and len(x.shape) == 1 and\n\u001b[0m\u001b[0;32m    266\u001b[0m       (expected_shape is None or len(expected_shape) != 1)):\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "y_pre = model.predict(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# print(y_pred)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where( y_pred != y_actual )[0]\n",
    "\n",
    "# 输出所有错误分类的索引\n",
    "len(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                     一个字--差4星收费0星service大堂 小孩当 小便--没人管向前 纸记号码头也不看就说没电视不能看说是电视公司在修不知当天会不会好早上吃早饭 去连坐的地方也没有别说吃的了住了2晚但酒店告诉 我只住了 该 的)   \n",
      "预测的分类 0\n",
      "实际的分类 [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 我们来找出错误分类的样本看看\n",
    "idx = misclassified[0]\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
