{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# 首先加载必用的库\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import gensim  用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0708 12:20:02.721931  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:05.682164  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:11.631563  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:15.867930  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:17.276091  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:17.751823  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:18.163806  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:18.195078  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:19.192272  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:19.313145  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:20.038809  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:20.182708  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:20.198330  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:20.684089  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:20.972956  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:20.980594  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.034449  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.074343  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.111248  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.185043  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.200700  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.216288  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.239071  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.289962  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.298876  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.349776  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.368758  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.368758  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.401068  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.409081  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.435008  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.450630  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.450630  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.450630  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.450630  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.450630  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.466251  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 12:20:21.466251  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.466251  7060 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.466251  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.481873  7060 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.481873  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.500180  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.501211  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.505167  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.507196  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.513178  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.519129  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.521127  7060 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.524117  7060 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.530099  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.531097  7060 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0708 12:20:21.534124  7060 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n"
     ]
    }
   ],
   "source": [
    "# 使用gensim加载预训练中文分词embedding, 有可能需要等待1-2分钟\n",
    "cn_model = KeyedVectors.load_word2vec_format('../embeddings/sgns.zhihu.bigram',binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由此可见每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['山东大学'].shape[0]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数：7909\n"
     ]
    }
   ],
   "source": [
    "# 获得样本的索引\n",
    "import pandas as pd\n",
    "\n",
    "data_neg = pd.read_excel('../data/neg.xlsx')\n",
    "data_pos = pd.read_excel('../data/pos.xls')\n",
    "\n",
    "print('样本总数：'+str(len(data_pos) + len(data_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。\n",
       "0  作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...                                                                                                                                                                                                             "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有的评价内容放置到一个list里\n",
    "train_texts_orig = []\n",
    "# 文本所对应的labels，也就是标记\n",
    "train_target = []\n",
    "\n",
    "for indexs in data_neg.index:\n",
    "    train_texts_orig.append(data_neg.loc[indexs].values[1])\n",
    "    train_target.append(data_neg.loc[indexs].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'明明在携程上用信用卡担保定了房，入住时居然前台说没有此信息，而且房间已经住满了，打电话到携程投诉，说是酒店预订部与前台没沟通好，折腾了半天，非常气人!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_texts_orig))\n",
    "# print(train_target)\n",
    "train_texts_orig[3824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indexs in data_pos.index:\n",
    "    train_texts_orig.append(data_pos.loc[indexs].values[0])\n",
    "    train_target.append(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'最真实与实用的白领成长故事，也是目前为止我所读过的最深刻的白领生活工作的总结。看了不但受益匪浅，而且觉得过瘾、振奋，为中国有这样优秀的打工人而自豪。看了确实有：“...让我回忆最多的，却不是人生旅途里这些成功的瞬间，而是那些刻骨铭心的失败或挫折...”它展示地是一个真实与真诚的人，一个真心与你分享如何才能成功，而不是一个天才的故事、打工的神仙。强烈推荐！'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_texts_orig))\n",
    "# print(train_target)\n",
    "train_texts_orig[3856]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "I0708 12:20:23.766945  7060 __init__.py:111] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache F:\\AppData\\summer\\Temp\\jieba.cache\n",
      "I0708 12:20:23.769938  7060 __init__.py:131] Loading model from cache F:\\AppData\\summer\\Temp\\jieba.cache\n",
      "Loading model cost 0.856 seconds.\n",
      "I0708 12:20:24.624858  7060 __init__.py:163] Loading model cost 0.856 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "I0708 12:20:24.625855  7060 __init__.py:164] Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有7909个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（） ]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens: [76 34 33 ... 39 16 19]\n",
      "7909\n"
     ]
    }
   ],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print(\"num_tokens:\",num_tokens)\n",
    "# print(train_tokens)\n",
    "len(train_tokens)\n",
    "print(len(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.6515362245543\n"
     ]
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "print(np.mean(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1540\n"
     ]
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "print(np.max(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins = 100)\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9675053736249842"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为223时，大约96%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'再'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.index2word[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59,\n",
       " 69,\n",
       " 0,\n",
       " 154664,\n",
       " 1,\n",
       " 2010,\n",
       " 473,\n",
       " 16,\n",
       " 1,\n",
       " 57,\n",
       " 2537,\n",
       " 1,\n",
       " 11119,\n",
       " 649,\n",
       " 1305,\n",
       " 144,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 19945,\n",
       " 869,\n",
       " 90,\n",
       " 86,\n",
       " 197235,\n",
       " 126,\n",
       " 155,\n",
       " 39283,\n",
       " 22,\n",
       " 18,\n",
       " 711,\n",
       " 3,\n",
       " 40,\n",
       " 155,\n",
       " 574,\n",
       " 1,\n",
       " 87,\n",
       " 1946,\n",
       " 8,\n",
       " 34,\n",
       " 655,\n",
       " 1,\n",
       " 644,\n",
       " 4253,\n",
       " 18,\n",
       " 38,\n",
       " 16,\n",
       " 143,\n",
       " 13176,\n",
       " 78,\n",
       " 49,\n",
       " 0,\n",
       " 51,\n",
       " 27,\n",
       " 135,\n",
       " 29865,\n",
       " 391,\n",
       " 18,\n",
       " 711,\n",
       " 3,\n",
       " 91,\n",
       " 51,\n",
       " 202,\n",
       " 39,\n",
       " 11119,\n",
       " 4862,\n",
       " 122,\n",
       " 62,\n",
       " 441,\n",
       " 1,\n",
       " 188009,\n",
       " 691,\n",
       " 613,\n",
       " 4,\n",
       " 374,\n",
       " 7294,\n",
       " 79]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'做为 声名在外的流行书说的还是广州的外企按道理应该和我的生存环境差不多啊但是一看之下才发现相去甚远这也就算了还发现其中的很多规则有很强的企业个性也就说只是个例而不是 给我们这些老油条看看也就算了如果给那些对外企向往或者想了解的freshman来看实在是容易误导他们'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse = reverse_tokens(train_tokens[0])\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只使用前50000个词\n",
    "num_words = 100000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.801784, -0.16534 ,  0.030508, ...,  0.106525,  0.553436,\n",
       "         0.43665 ],\n",
       "       [-0.651747,  0.53597 ,  0.340271, ...,  0.805399,  0.104593,\n",
       "         0.193694],\n",
       "       [-0.412321,  0.228261,  0.207114, ...,  0.808777,  0.056751,\n",
       "         0.452374],\n",
       "       ...,\n",
       "       [ 0.399163, -0.361203, -0.155123, ..., -0.026335,  0.078385,\n",
       "         0.042472],\n",
       "       [-0.198514, -0.436683,  0.416873, ..., -0.104323,  0.072952,\n",
       "         0.390674],\n",
       "       [ 0.071814,  0.090673,  0.014804, ..., -0.278364, -0.241816,\n",
       "        -0.227115]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum(cn_model[cn_model.index2word[30]] == embedding_matrix[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix.shape (100000, 300)\n"
     ]
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "print(\"embedding_matrix.shape\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0, 18229,\n",
       "           1,   832,    18,   447,  1357,   421,  1148,  9827,     1,\n",
       "        8108,     1,    34,  8621,   203,    10,   779,     0,  1087,\n",
       "          86,  1357,    57,  4593,    86,  1704,    24,    96,  7737,\n",
       "           1, 13729,   112, 57734,    34,   369,   989])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens, padding='pre', truncating='pre')\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[train_pad >= num_words] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pad[33] [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0 18229     1   832    18\n",
      "   447  1357   421  1148  9827     1  8108     1    34  8621   203    10\n",
      "   779     0  1087    86  1357    57  4593    86  1704    24    96  7737\n",
      "     1 13729   112 57734    34   369   989]\n"
     ]
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "print(\"train_pad[33]\", train_pad[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '1', ..., '0', '0', '0'], dtype='<U21')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target向量\n",
    "train_target = np.array(train_target)\n",
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\"\"\" one-hot处理标签 \"\"\"\n",
    "train_target = to_categorical(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7909, 10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, train_target, test_size=0.1, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize_sequences(squences, dimension=10):\n",
    "#     \"\"\"\n",
    "#     @函数功能:将序列向量化，初始化全0的序列，在单词索引对应的位置上置1\n",
    "#     \"\"\"\n",
    "#     resluts = np.zeros((len(squences), dimension))\n",
    "#     for i, sequence in enumerate(squences):\n",
    "#         resluts[i, sequence] = 1\n",
    "#     return resluts\n",
    "\n",
    "# X_train = vectorize_sequences(X_train)\n",
    "# X_test = vectorize_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                              很好的酒店很喜欢房间很干净很漂亮从房间的窗口看出去超美的在市中心区域出行也非常的方便有机会一定会再住的\n",
      "class:  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[30]))\n",
    "print('class: ', y_train[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 12:20:32.123103  7060 deprecation.py:506] From C:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0708 12:20:32.640611  7060 deprecation.py:506] From C:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 模型第一层为embedding\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_tokens, trainable=False))\n",
    "\n",
    "# model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "# model.add(LSTM(units=16, return_sequences=False))\n",
    "model.add(LSTM(units=32, return_sequences=False))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 223, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 30,042,954\n",
      "Trainable params: 42,954\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 我们来看一下模型的结构，一共90k左右可训练的变量\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open file (unable to open file: name = 'sentiment_checkpoint_Class10_V1.0.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint_Class10_V1.0.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "\n",
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-8, patience=0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping,\n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 12:20:33.130766  7060 deprecation.py:323] From C:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6406 samples, validate on 712 samples\n",
      "Epoch 1/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 1.5640 - acc: 0.5094\n",
      "Epoch 00001: val_loss improved from inf to 1.25421, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 17s 3ms/sample - loss: 1.5637 - acc: 0.5092 - val_loss: 1.2542 - val_acc: 0.5857\n",
      "Epoch 2/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 1.1014 - acc: 0.6442\n",
      "Epoch 00002: val_loss improved from 1.25421 to 0.93306, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 15s 2ms/sample - loss: 1.1021 - acc: 0.6441 - val_loss: 0.9331 - val_acc: 0.6728\n",
      "Epoch 3/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.9073 - acc: 0.6945\n",
      "Epoch 00003: val_loss improved from 0.93306 to 0.81526, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 15s 2ms/sample - loss: 0.9071 - acc: 0.6945 - val_loss: 0.8153 - val_acc: 0.7205\n",
      "Epoch 4/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.8304 - acc: 0.7158\n",
      "Epoch 00004: val_loss improved from 0.81526 to 0.76981, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 15s 2ms/sample - loss: 0.8305 - acc: 0.7159 - val_loss: 0.7698 - val_acc: 0.7402\n",
      "Epoch 5/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.7593 - acc: 0.7355\n",
      "Epoch 00005: val_loss did not improve from 0.76981\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6406/6406 [==============================] - 18s 3ms/sample - loss: 0.7595 - acc: 0.7354 - val_loss: 0.8943 - val_acc: 0.6728\n",
      "Epoch 6/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.7560 - acc: 0.7330\n",
      "Epoch 00006: val_loss improved from 0.76981 to 0.72756, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 16s 3ms/sample - loss: 0.7560 - acc: 0.7331 - val_loss: 0.7276 - val_acc: 0.7500\n",
      "Epoch 7/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6981 - acc: 0.7583\n",
      "Epoch 00007: val_loss improved from 0.72756 to 0.71127, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 16s 3ms/sample - loss: 0.6977 - acc: 0.7585 - val_loss: 0.7113 - val_acc: 0.7640\n",
      "Epoch 8/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6848 - acc: 0.7625\n",
      "Epoch 00008: val_loss improved from 0.71127 to 0.70746, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 29s 4ms/sample - loss: 0.6853 - acc: 0.7624 - val_loss: 0.7075 - val_acc: 0.7626\n",
      "Epoch 9/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6792 - acc: 0.7680\n",
      "Epoch 00009: val_loss improved from 0.70746 to 0.70098, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 31s 5ms/sample - loss: 0.6788 - acc: 0.7682 - val_loss: 0.7010 - val_acc: 0.7725\n",
      "Epoch 10/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6713 - acc: 0.7691\n",
      "Epoch 00010: val_loss improved from 0.70098 to 0.69929, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 30s 5ms/sample - loss: 0.6715 - acc: 0.7690 - val_loss: 0.6993 - val_acc: 0.7697\n",
      "Epoch 11/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6661 - acc: 0.7727\n",
      "Epoch 00011: val_loss improved from 0.69929 to 0.69661, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 30s 5ms/sample - loss: 0.6659 - acc: 0.7729 - val_loss: 0.6966 - val_acc: 0.7725\n",
      "Epoch 12/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6599 - acc: 0.7745\n",
      "Epoch 00012: val_loss improved from 0.69661 to 0.69434, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 30s 5ms/sample - loss: 0.6601 - acc: 0.7743 - val_loss: 0.6943 - val_acc: 0.7711\n",
      "Epoch 13/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6541 - acc: 0.7758\n",
      "Epoch 00013: val_loss improved from 0.69434 to 0.69274, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 30s 5ms/sample - loss: 0.6546 - acc: 0.7755 - val_loss: 0.6927 - val_acc: 0.7753\n",
      "Epoch 14/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6495 - acc: 0.7803\n",
      "Epoch 00014: val_loss improved from 0.69274 to 0.68955, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 31s 5ms/sample - loss: 0.6493 - acc: 0.7804 - val_loss: 0.6896 - val_acc: 0.7725\n",
      "Epoch 15/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6433 - acc: 0.7834\n",
      "Epoch 00015: val_loss improved from 0.68955 to 0.68504, saving model to sentiment_checkpoint_Class10_V1.0.keras\n",
      "6406/6406 [==============================] - 31s 5ms/sample - loss: 0.6433 - acc: 0.7833 - val_loss: 0.6850 - val_acc: 0.7753\n",
      "Epoch 16/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6588 - acc: 0.7753\n",
      "Epoch 00016: val_loss did not improve from 0.68504\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6406/6406 [==============================] - 31s 5ms/sample - loss: 0.6587 - acc: 0.7752 - val_loss: 0.7144 - val_acc: 0.7528\n",
      "Epoch 17/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.7706\n",
      "Epoch 00017: val_loss did not improve from 0.68504\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "6406/6406 [==============================] - 31s 5ms/sample - loss: 0.6664 - acc: 0.7707 - val_loss: 0.7030 - val_acc: 0.7683\n",
      "Epoch 18/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6600 - acc: 0.7728\n",
      "Epoch 00018: val_loss did not improve from 0.68504\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "6406/6406 [==============================] - 31s 5ms/sample - loss: 0.6599 - acc: 0.7729 - val_loss: 0.7024 - val_acc: 0.7683\n",
      "Epoch 19/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.7728\n",
      "Epoch 00019: val_loss did not improve from 0.68504\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "6406/6406 [==============================] - 35s 5ms/sample - loss: 0.6595 - acc: 0.7729 - val_loss: 0.7024 - val_acc: 0.7683\n",
      "Epoch 20/20\n",
      "6400/6406 [============================>.] - ETA: 0s - loss: 0.6594 - acc: 0.7728\n",
      "Epoch 00020: val_loss did not improve from 0.68504\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "6406/6406 [==============================] - 21s 3ms/sample - loss: 0.6594 - acc: 0.7729 - val_loss: 0.7024 - val_acc: 0.7683\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "history = model.fit(X_train, y_train,\n",
    "          validation_split=0.1,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.475465745078077,\n",
       "  1.0321501104525763,\n",
       "  0.9788035252784291,\n",
       "  0.877032354776464,\n",
       "  0.8218140274221138,\n",
       "  0.7594383644324484,\n",
       "  0.7160257967633826,\n",
       "  0.6894514513618679,\n",
       "  0.6775672882934901,\n",
       "  0.6701676432802794,\n",
       "  0.660525131822935,\n",
       "  0.6540994528603115,\n",
       "  0.645686572928783,\n",
       "  0.639702994135667,\n",
       "  0.6360668492920132,\n",
       "  0.6351471176714664,\n",
       "  0.6350428458213657,\n",
       "  0.6350340524242328,\n",
       "  0.6350335110882614],\n",
       " 'acc': [0.55775833,\n",
       "  0.66359663,\n",
       "  0.67311895,\n",
       "  0.7129254,\n",
       "  0.71557915,\n",
       "  0.7343116,\n",
       "  0.74898535,\n",
       "  0.7597565,\n",
       "  0.76319075,\n",
       "  0.7642835,\n",
       "  0.77005935,\n",
       "  0.76724946,\n",
       "  0.77958167,\n",
       "  0.78067434,\n",
       "  0.7817671,\n",
       "  0.7819232,\n",
       "  0.7820793,\n",
       "  0.7820793,\n",
       "  0.7820793],\n",
       " 'val_loss': [1.0712110072039487,\n",
       "  1.0128524410590698,\n",
       "  0.8793537395723751,\n",
       "  0.8382323498136541,\n",
       "  0.783096975155091,\n",
       "  0.8094800027568688,\n",
       "  0.7189175352621614,\n",
       "  0.7082371684942352,\n",
       "  0.7057914760675323,\n",
       "  0.6998210964577921,\n",
       "  0.6958600577343715,\n",
       "  0.6927456795499566,\n",
       "  0.697834849357605,\n",
       "  0.6906361506226357,\n",
       "  0.6908078655767976,\n",
       "  0.6908492425854287,\n",
       "  0.6908408523945326,\n",
       "  0.6908407633224231,\n",
       "  0.6908412595813194],\n",
       " 'val_acc': [0.6292135,\n",
       "  0.64606744,\n",
       "  0.6994382,\n",
       "  0.6994382,\n",
       "  0.7078652,\n",
       "  0.70926964,\n",
       "  0.73876405,\n",
       "  0.7429775,\n",
       "  0.7373595,\n",
       "  0.7233146,\n",
       "  0.72893256,\n",
       "  0.7275281,\n",
       "  0.7457865,\n",
       "  0.75,\n",
       "  0.7485955,\n",
       "  0.7485955,\n",
       "  0.7485955,\n",
       "  0.7485955,\n",
       "  0.7485955],\n",
       " 'lr': [0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.000100000005,\n",
       "  0.000100000005,\n",
       "  0.000100000005,\n",
       "  0.000100000005,\n",
       "  0.000100000005,\n",
       "  0.000100000005,\n",
       "  0.000100000005,\n",
       "  1.0000001e-05,\n",
       "  1.0000001e-05,\n",
       "  1.0000001e-06,\n",
       "  1.0000001e-07,\n",
       "  1.0000001e-08,\n",
       "  1e-08]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_acc(history):\n",
    "    \"\"\" 绘制精度曲线 \"\"\"\n",
    "    plt.clf()\n",
    "    history_dict = history.history\n",
    "    acc = history_dict['acc']\n",
    "    val_acc = history_dict['val_acc']\n",
    "\n",
    "    epochs = range(1, len(val_acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" show result \"\"\"\n",
    "show_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791/791 [==============================] - 1s 1ms/sample - loss: 0.7750 - acc: 0.7345\n",
      "Accuracy:73.45%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#开始测试\n",
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens, padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    num = np.argmax(result)\n",
    "    if num == 0:\n",
    "        print('是一例正面评价', 'output=%.2f' % result[0][num])\n",
    "    elif num >= 1 and num <= 9:\n",
    "        print('负面评价  %d' % d, 'output=%.2f' % result[0][num])\n",
    "    else:\n",
    "        print('error!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22222222\n"
     ]
    }
   ],
   "source": [
    "num = 2\n",
    "if num == 2:\n",
    "    print(\"22222222\")\n",
    "else:\n",
    "    print(\"55555555\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度很不好\n",
      "负面评价  2 output=0.17\n",
      "酒店卫生条件非常不好\n",
      "负面评价  6 output=0.33\n",
      "床铺非常舒适\n",
      "是一例正面评价 output=0.65\n",
      "房间很凉，不给开暖气\n",
      "负面评价  7 output=0.39\n",
      "房间很凉爽，空调冷气很足\n",
      "是一例正面评价 output=0.69\n",
      "酒店环境不好，住宿体验很不好\n",
      "负面评价  6 output=0.40\n",
      "房间隔音不到位\n",
      "负面评价  7 output=0.40\n",
      "晚上回来发现没有打扫卫生\n",
      "负面评价  7 output=0.28\n",
      "因为过节所以要我临时加钱，比团购的价格贵\n",
      "负面评价  7 output=0.38\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度很不好',\n",
    "    '酒店卫生条件非常不好',\n",
    "    '床铺非常舒适',\n",
    "    '房间很凉，不给开暖气',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生',\n",
    "    '因为过节所以要我临时加钱，比团购的价格贵'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店太脏且设施陈旧，不符合三星标准，前台价格低于携程价格\n",
      "负面评价  7 output=0.44\n"
     ]
    }
   ],
   "source": [
    "text = \"酒店太脏且设施陈旧，不符合三星标准，前台价格低于携程价格\"\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# print(y_pred)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where( y_pred != y_actual )[0]\n",
    "\n",
    "# 输出所有错误分类的索引\n",
    "len(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                   比较适合国内客一楼餐厅风格上有点南洋的味道\n",
      "预测的分类 1\n",
      "实际的分类 1\n"
     ]
    }
   ],
   "source": [
    "# 我们来找出错误分类的样本看看\n",
    "idx = misclassified[0]\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
