{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 19:19:19.501292 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:22.224977 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:27.332360 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:31.160092 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:32.505496 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:32.842595 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:33.211643 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:33.240533 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:34.212964 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:34.332613 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:34.802395 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:34.945974 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:34.963926 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:35.600257 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:35.880474 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:35.887457 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:35.949291 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:35.989185 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.028080 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.111891 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.134796 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.137789 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.164717 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.207638 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.217576 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.275459 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.306336 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.309330 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.338252 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.348226 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.385126 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.405074 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.409066 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.412055 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.419037 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.425021 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:19:36.426021 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.431004 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.438982 11792 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.441975 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.447958 11792 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.449954 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.462918 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.463917 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.469902 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.470897 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.479876 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.487852 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.490845 11792 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.493838 11792 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.501815 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.503810 11792 utils_any2vec.py:185] duplicate word '--------------------------------------------------------------------------------------------------' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.506803 11792 utils_any2vec.py:185] duplicate word '..................................................................................................' in ../embeddings/sgns.zhihu.bigram, ignoring all but first\n",
      "W0709 19:19:36.525753 11792 deprecation.py:506] From C:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0709 19:19:37.140108 11792 deprecation.py:506] From C:\\SoftWare\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "import jieba\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Embedding\n",
    "import numpy as np\n",
    "\n",
    "# 使用gensim加载预训练中文分词embedding, 有可能需要等待1-2分钟\n",
    "cn_model = KeyedVectors.load_word2vec_format('../embeddings/sgns.zhihu.bigram', binary=False, unicode_errors=\"ignore\")\n",
    "\n",
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()\n",
    "\n",
    "# 由此可见每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['山东大学'].shape[0]\n",
    "\n",
    "# 只使用前50000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# 模型第一层为embedding\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=223, trainable=False))\n",
    "model.add(LSTM(units=32, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 建立一个权重的存储点\n",
    "path_checkpoint = './sentiment_checkpoint_Class2.keras'\n",
    "model.load_weights(path_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'comment': 'Zara', 'percent': 7}\n",
    "def predict_sentiment(text):\n",
    "    #print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [i for i in cut]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=223, padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    return int((result[0][0])*100)\n",
    "#     if coef >= 0.5:\n",
    "#         dict['comment'] = \"\"\n",
    "#         dict['percent'] = \n",
    "#         #print('是一例正面评价', 'output=%.2f' % coef)\n",
    "#     else:\n",
    "#         #print('是一例负面评价', 'output=%.2f' % coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"酒店很好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "就带\n"
     ]
    }
   ],
   "source": [
    "print(\"就带\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-b7f8ea9afadd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ms\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m0.23\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "s = \"\"\n",
    "s += 0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'comment': \"demo\", 'percent': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict['comment'] = \"asdlfk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdlfk'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict['percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
